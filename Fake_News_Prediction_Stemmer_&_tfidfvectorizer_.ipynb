{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fake News Prediction Stemmer & tfidfvectorizer .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOS7F86K+gLk9gC4PWP+uYe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codekennML/FakeNews-Detection/blob/main/Fake_News_Prediction_Stemmer_%26_tfidfvectorizer_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Algorithm will aim to distinguish between news (Fake & Real) \n",
        "Using Tfidf Vectorizer"
      ],
      "metadata": {
        "id": "9FB71sFYem0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile \n",
        "with ZipFile('/content/train.csv.zip', 'r') as zp:\n",
        " zp.extractall()\n",
        " print('File Extraction done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNPWpyTwe-b8",
        "outputId": "678f4ae6-51dc-499e-f532-c2f7bd743f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Extraction done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset Attributes \n",
        "\n",
        "#  train.csv: A full training dataset with the following attributes:\n",
        "\n",
        "#  id: unique id for a news article\n",
        "#  title: the title of a news article\n",
        "#  author: author of the news article\n",
        "#  text: the text of the article; could be incomplete\n",
        "#  label: a label that marks the article as potentially unreliable\n",
        "#  1: unreliable\n",
        "#  0: reliable"
      ],
      "metadata": {
        "id": "_mZhp3pAnvU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Dependencies \n",
        "\n",
        "import  numpy as np \n",
        "import pandas as pd\n",
        "import re \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.stem.porter import PorterStemmer \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score \n",
        "\n",
        "print('Dependencies Loaded, LFG...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf1qF1ZQoKER",
        "outputId": "94219a2d-f713-4818-a112-f20e01fb977f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencies Loaded, LFG...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cZKdrUdXnnpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the nltk stopwords \n",
        "#Stopwords are characters or phrases that do not add or subtract extensive meaning from \n",
        "# a statement or sentence e.g \"The, and , etc \"\n",
        "\n",
        "import nltk \n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZAqrilOoHdQ",
        "outputId": "2ef9ceba-34b6-47f0-d155-dcd727cf4fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets print the stopwords , in english \n",
        "\n",
        "print(stopwords.words('english'))\n",
        "\n",
        "#As discussed earlier , the stopwords are seen below "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAaAAV3jqsFo",
        "outputId": "3e0df28a-28bf-465a-8e5b-8bae12182cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets do some data preprocessing \n",
        "#Load the data into a pandas dataframe\n",
        "\n",
        "df =  pd.read_csv('/content/train.csv')\n",
        "\n",
        "print('Dataset loaded sucessfully ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imt8Qd3ZqVUz",
        "outputId": "2bb11b9b-9e36-4f5e-b2c9-b518cb18256a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded sucessfully \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the number of rows and columns \n",
        "\n",
        "df.shape\n",
        "\n",
        "#The dataset has 20800 rows and 5 columns "
      ],
      "metadata": {
        "id": "fs6DvJtFsTrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets visualize the columns \n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "9O0aNY-esbSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets check the number of missing values in the dataset \n",
        "\n",
        "df.isnull().sum()\n",
        "\n",
        "#558 authors are missing , 1957 authors missing and 39 texts are missing \n",
        "\n",
        "#Since the number of missing values are a bit negliglibe and their disposal would not adversely affect the training "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhHuB_V5sn_c",
        "outputId": "f16ae56f-957e-41b1-b4b4-72a1e1ceca86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id           0\n",
              "title      558\n",
              "author    1957\n",
              "text        39\n",
              "label        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets replace the null values with an empty string \n",
        "\n",
        "df = df.fillna('')\n",
        "\n"
      ],
      "metadata": {
        "id": "5y7z4Mvds3xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9KllsaBdNf8",
        "outputId": "8b414881-4e5e-4b8a-bbf3-69b393d2ef2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id        0\n",
              "title     0\n",
              "author    0\n",
              "text      0\n",
              "label     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#A bit of feature selection - Merging title and author \n",
        "df['content'] = df['author']+ ' '+df['title']\n",
        "print(df['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJ0pG2xmtn25",
        "outputId": "c3b578ac-6dca-4796-9cfd-fe48372ef927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        Darrell Lucus House Dem Aide: We Didn’t Even S...\n",
            "1        Daniel J. Flynn FLYNN: Hillary Clinton, Big Wo...\n",
            "2        Consortiumnews.com Why the Truth Might Get You...\n",
            "3        Jessica Purkiss 15 Civilians Killed In Single ...\n",
            "4        Howard Portnoy Iranian woman jailed for fictio...\n",
            "                               ...                        \n",
            "20795    Jerome Hudson Rapper T.I.: Trump a ’Poster Chi...\n",
            "20796    Benjamin Hoffman N.F.L. Playoffs: Schedule, Ma...\n",
            "20797    Michael J. de la Merced and Rachel Abrams Macy...\n",
            "20798    Alex Ansary NATO, Russia To Hold Parallel Exer...\n",
            "20799              David Swanson What Keeps the F-35 Alive\n",
            "Name: content, Length: 20800, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Seperating the data into features and targets \n",
        "x = df.drop(['label'], axis =  1 )\n",
        "y = df['label']\n",
        "\n"
      ],
      "metadata": {
        "id": "XOW4i80duO0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we proceed to stemming the data , Stemming is the process of reducing a word to its root \n",
        "# acting , actor -> act\n",
        "\n",
        "porter_stem =  PorterStemmer()\n",
        "  # defining the stemming function \n",
        "\n",
        "def stemming(content):\n",
        "    stemmed_content = re.sub('[^a-zA-Z]',' ',str(content))\n",
        "    stemmed_content = stemmed_content.lower()\n",
        "    stemmed_content = stemmed_content.split()\n",
        "    stemmed_content = [porter_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
        "    stemmed_content = ' '.join(stemmed_content)\n",
        "    return stemmed_content\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RGL7ro3svmm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying our just defined function to the content column of our df dataset\n",
        "\n",
        "df['content'] =  df['content'].apply(stemming)"
      ],
      "metadata": {
        "id": "qKU0lOqY1hjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the stemmed content \n",
        "print(df['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "116JSp4n2WDw",
        "outputId": "fdfecc3e-9318-4e8f-99ca-f112f2657b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        darrel lucu hous dem aid even see comey letter...\n",
            "1        daniel j flynn flynn hillari clinton big woman...\n",
            "2                   consortiumnew com truth might get fire\n",
            "3        jessica purkiss civilian kill singl us airstri...\n",
            "4        howard portnoy iranian woman jail fiction unpu...\n",
            "                               ...                        \n",
            "20795    jerom hudson rapper trump poster child white s...\n",
            "20796    benjamin hoffman n f l playoff schedul matchup...\n",
            "20797    michael j de la merc rachel abram maci said re...\n",
            "20798    alex ansari nato russia hold parallel exercis ...\n",
            "20799                            david swanson keep f aliv\n",
            "Name: content, Length: 20800, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = df['content'].values\n",
        "y =  df['label'].values"
      ],
      "metadata": {
        "id": "dCqBcOE25sTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ej9aTa32etY7",
        "outputId": "2ee1b791-77ae-4a6f-caa9-975ec6d10f41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['darrel lucu hous dem aid even see comey letter jason chaffetz tweet'\n",
            " 'daniel j flynn flynn hillari clinton big woman campu breitbart'\n",
            " 'consortiumnew com truth might get fire' ...\n",
            " 'michael j de la merc rachel abram maci said receiv takeov approach hudson bay new york time'\n",
            " 'alex ansari nato russia hold parallel exercis balkan'\n",
            " 'david swanson keep f aliv'] [1 0 1 ... 0 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Since our model cannot process text inputs\n",
        "#We have to convert the textual data into numerical data using tfidfVectorizer\n",
        "\n",
        "text_to_num =  TfidfVectorizer()\n",
        "text_to_num.fit(x)\n",
        "\n",
        "x_transfmd = text_to_num.transform(x)\n"
      ],
      "metadata": {
        "id": "j6E5bk_0evS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print x \n",
        "print(x_transfmd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hmjjjz-ZgAz9",
        "outputId": "c29ceb78-ae2c-4d99-c51c-e24c77c4fd2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 15686)\t0.28485063562728646\n",
            "  (0, 13473)\t0.2565896679337957\n",
            "  (0, 8909)\t0.3635963806326075\n",
            "  (0, 8630)\t0.29212514087043684\n",
            "  (0, 7692)\t0.24785219520671603\n",
            "  (0, 7005)\t0.21874169089359144\n",
            "  (0, 4973)\t0.233316966909351\n",
            "  (0, 3792)\t0.2705332480845492\n",
            "  (0, 3600)\t0.3598939188262559\n",
            "  (0, 2959)\t0.2468450128533713\n",
            "  (0, 2483)\t0.3676519686797209\n",
            "  (0, 267)\t0.27010124977708766\n",
            "  (1, 16799)\t0.30071745655510157\n",
            "  (1, 6816)\t0.1904660198296849\n",
            "  (1, 5503)\t0.7143299355715573\n",
            "  (1, 3568)\t0.26373768806048464\n",
            "  (1, 2813)\t0.19094574062359204\n",
            "  (1, 2223)\t0.3827320386859759\n",
            "  (1, 1894)\t0.15521974226349364\n",
            "  (1, 1497)\t0.2939891562094648\n",
            "  (2, 15611)\t0.41544962664721613\n",
            "  (2, 9620)\t0.49351492943649944\n",
            "  (2, 5968)\t0.3474613386728292\n",
            "  (2, 5389)\t0.3866530551182615\n",
            "  (2, 3103)\t0.46097489583229645\n",
            "  :\t:\n",
            "  (20797, 13122)\t0.2482526352197606\n",
            "  (20797, 12344)\t0.27263457663336677\n",
            "  (20797, 12138)\t0.24778257724396507\n",
            "  (20797, 10306)\t0.08038079000566466\n",
            "  (20797, 9588)\t0.174553480255222\n",
            "  (20797, 9518)\t0.2954204003420313\n",
            "  (20797, 8988)\t0.36160868928090795\n",
            "  (20797, 8364)\t0.22322585870464118\n",
            "  (20797, 7042)\t0.21799048897828688\n",
            "  (20797, 3643)\t0.21155500613623743\n",
            "  (20797, 1287)\t0.33538056804139865\n",
            "  (20797, 699)\t0.30685846079762347\n",
            "  (20797, 43)\t0.29710241860700626\n",
            "  (20798, 13046)\t0.22363267488270608\n",
            "  (20798, 11052)\t0.4460515589182236\n",
            "  (20798, 10177)\t0.3192496370187028\n",
            "  (20798, 6889)\t0.32496285694299426\n",
            "  (20798, 5032)\t0.4083701450239529\n",
            "  (20798, 1125)\t0.4460515589182236\n",
            "  (20798, 588)\t0.3112141524638974\n",
            "  (20798, 350)\t0.28446937819072576\n",
            "  (20799, 14852)\t0.5677577267055112\n",
            "  (20799, 8036)\t0.45983893273780013\n",
            "  (20799, 3623)\t0.37927626273066584\n",
            "  (20799, 377)\t0.5677577267055112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the dataset into training and test data \n",
        "\n",
        "x_train, x_test, y_train, y_test =  train_test_split(x_transfmd,y,test_size = 0.2,stratify = y, random_state = 2)"
      ],
      "metadata": {
        "id": "l6cdMN03gJKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training the model \n",
        "LogReg =  LogisticRegression()\n",
        "\n",
        "training_result  =  LogReg.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "PzFKGkMChNme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model evaluation \n",
        "y_predict =  LogReg.predict(x_test)\n",
        "accuracy = accuracy_score(y_test, y_predict)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLBWiWZPg8HY",
        "outputId": "059ebe15-2923-4367-b80c-85a4cac58a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9790865384615385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Making our prediction \n",
        "\n",
        "x_new =  x_test[0]\n",
        "prediction = LogReg.predict(x_new)\n",
        "\n",
        "if (prediction[0] == 0):\n",
        "  print ('The News is Real ')\n",
        "else: \n",
        "    print('Fake News!!!!!')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2lNo44JiiWx",
        "outputId": "ef4ce0cb-378a-4ca9-e75d-72c10fae50ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fake News!!!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HLOUbBhKjyGt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}